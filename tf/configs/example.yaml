%YAML 1.2
--- 
name: "4x128-cpu"
gpu: 0
dataset:
  num_chunks: 500_000_000
  allow_less_chunks: true
  train_ratio: 0.95
  sort_type: name
  input: 
    - '/Volumes/HP_P900/Users/tungnguyen/Programming/chessformer/lczero-training/data/train/*/'
  # input_train: 
  #   - '/Volumes/HP_P900/Users/tungnguyen/Programming/chessformer/lczero-training/data/train/*/'
  # input_test:
  #   - '/Volumes/HP_P900/Users/tungnguyen/Programming/chessformer/lczero-training/data/test/*/'
  # input_validation:
  # #  - '/mnt/data/validation-rescored/'
  train_workers: 2
  test_workers: 1
  fast_chunk_loading: false
  # pc_min: 0
  # pc_max: 6
training:
    precision: single    # single precision (float32) for CPU/MPS compatibility
    swa: false
    swa_output: false  
    swa_max_n: 10
    swa_steps: 100
    max_grad_norm: 10.0
    batch_size: 256     # reduced for lower RAM
    num_batch_splits: 1
    value_focus_min: 1.0
    value_focus_slope: 0.0
    lookahead_optimizer: false
    renorm: true
    renorm_max_r: 1.0
    renorm_max_d: 0.0
    test_steps: 1_000
    disable_pb_checkpointing: false
    checkpoint_activations: false

    # validation_steps: 5000
    num_test_positions: 8_192
    train_avg_report_steps: 100
    total_steps: 10_000       # reduced for testing
    checkpoint_steps: 1_000
    shuffle_size: 100_000     # reduced for lower RAM
    warmup_steps: 100
    mask_legal_moves: true
    lr_values: 
        - 0.001
        - 0.000316
        - 0.0001
    lr_boundaries:
        - 8_000
        - 9_000
    loss_weights:
        policy: 1.0
        policy_optimistic_st: 0.0
        policy_soft: 8.0 # loss is much smaller at optimum
        policy_opponent: 0.00
        policy_next: 0.00
        value_winner: 1.0
        value_q: 1.0
        value_q_cat: 0.1
        value_st: 1.0 # larger because mse loss
        value_q_err: 1.0  # both error weights should be the same
        value_st_err: 1.0
        value_st_cat: 0.1
        moves_left: 1.0
        reg: 1.0
    path: 'networks'

    optimizer: nadam # sgd/nadam/rmsprop/adabelief/adam
    beta_1: 0.9 
    beta_2: 0.98 
    epsilon: 0.00000001 # 1e-7
    sparse: false

    return_attn_wts: false
    return_activations: false

model:

    # Dimension parameters (reduced for CPU/low RAM)
    embedding_size: 128
    policy_embedding_size: 128
    value_embedding_size: 32
    moves_left_embedding_size: 32
    encoder_layers: 4                    # reduced from 8
    encoder_heads: 4                     # reduced from 8
    encoder_d_model: 128                 # reduced from 256
    encoder_dff: 128                     # reduced from 256
    policy_d_model: 128                  # reduced from 256
    policy_d_aux: 128                    # reduced from 256
    dropout_rate: 0.0                    # the dropout rate used for weight regularization of attention during training
                                        # makes memory 33 -> 39 GB on A100 as observed by Teck and Kovax

    embedding_style: "old"
    embedding_dense_sz: 32

    value: 'wdl'
    moves_left: 'v1'
    input_type: 'classic'

    # Smolgen
    use_smolgen: false
    smolgen_hidden_channels: 16
    smolgen_hidden_sz: 64
    smolgen_gen_sz: 64
    smolgen_activation: 'swish'

    # RPE
    use_rpe_q: true
    use_rpe_k: true
    use_rpe_v: true

    # Gating
    use_logit_gating: false

    # Ablations
    omit_qkv_biases: true # these two increases training speed by ~10% on BT4 
    encoder_rms_norm: true # without quality degradation

    # Output heads
    policy_optimistic_st: false
    policy_opponent: false
    policy_next: false
    value_st: true
    value_q: true
    soft_policy: true
    categorical_value_buckets: 32
    soft_policy_temperature: 4.0

    # quantization
    quantize_activations: false
    quantize_weights: false
    quantize_activation_bits: 8
    quantize_weight_bits: 8
    quantize_channels: false
    rep_quant: false
